{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DL Assignment 3 - Recurrent Neural Networks for Image Classification\n",
        "\n",
        "Welcome to the **third graded assignment** of the DL course. In the last labs we were using recurrent neural networks (RNNs) for next character and word prediction, text generation and sequence classification. So far, all these tasks were bound to natural language. Of course, RNNs can also process other types of data when presented as a sequence. In this assignment, you will use RNNs for **image classification**. In detail, you will build a **pixel wise** and a **row wise RNN** based on a **long short-term memory (LSTM)** and train them to classify digits from an image.\n",
        "\n",
        "\n",
        "**Instructions**\n",
        "- You'll be using Python 3 in the iPython based Google Colaboratory\n",
        "- Lines encapsulated in \"<font color='green'>`### START YOUR CODE HERE ###`</font>\" and \"<font color='green'>`### END YOUR CODE HERE ###`</font>\", or marked by \"<font color='green'>`# TODO`</font>\", denote the code fragments to be completed by you.\n",
        "- There's no need to write any other code.\n",
        "- After writing your code, you can run the cell by either pressing `SHIFT`+`ENTER` or by clicking on the play symbol on the left side of the cell.\n",
        "- We may specify \"<font color='green'>`(≈ X LOC)`</font>\" to tell you about how many lines of code you need to write. This is just a rough estimate, so don't feel bad if your code is longer or shorter.\n",
        "\n",
        "**Much success!**\n",
        "\n",
        "***\n",
        "\n",
        "<font color='darkblue'>\n",
        "  \n",
        "**Remember**  \n",
        "- Run your cells using `SHIFT`+`ENTER` (or \"Run cell\")\n",
        "- Write code in the designated areas using Python 3 only\n",
        "- Do not modify the code outside of the designated areas\n",
        "- Do not import/use any other packages. Code relying on packages other than the provided ones won't be graded.\n",
        "- Activate GPU acceleration by clicking `Runtime` -> `Change runtime type` and select `GPU` from the dropdown menu entitled `Hardware accelerator`\n",
        "</font>\n",
        "\n",
        "***\n",
        "\n",
        "<font color='red'>\n",
        "  \n",
        "**Note**  \n",
        "You have to develop and submit your own solution. If we have reasons to believe you shared or did not submit your own work, we have to consider an attempted fraud. In this case your submission will be graded zero points and we reserve additional measures.\n",
        "</font>\n",
        "\n",
        "***"
      ],
      "metadata": {
        "id": "KQPv5E-r8gah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0 - Preparation"
      ],
      "metadata": {
        "id": "BS3AT7h_Wr11"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and Test for GPU\n",
        "\n",
        "Execute the cells below to import the required modules and test for GPU availability:"
      ],
      "metadata": {
        "id": "v1PMuH3FVy0a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgT6gWgkplm8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "!wget -nv -t 0 --show-progress https://cloud.tu-ilmenau.de/s/BpMm4QJpMqaiakS/download/utils.py\n",
        "\n",
        "import utils"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Print TF version and GPU stats\n",
        "print('TensorFlow version:', tf.__version__)\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "   raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name), '', sep='\\n')\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "oInbWRtikjwL",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 - Data Preparation"
      ],
      "metadata": {
        "id": "-aflZm9PdEt7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At first we need to load and prepare the dataset. In this assignment, you will be using the [MNIST database of handwritten digits](https://www.tensorflow.org/datasets/catalog/mnist). You will use [TensorFlow Datasets](https://www.tensorflow.org/datasets/overview) (`tdfs`) to load the data. The `tdfs` is a collection of ready-to-use datasets and conveniently handled as a utility module.\n",
        "\n",
        "The next cell uses [`tfds.load`](https://www.tensorflow.org/datasets/overview#tfdsload) to load the train and test splits of MNIST as *supervised* dataset, i.e. with labels."
      ],
      "metadata": {
        "id": "rjbdCBkJc0Ve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "def load_mnist():\n",
        "  \n",
        "  splits = tfds.load(\n",
        "      'mnist',\n",
        "      split=['train', 'test'],\n",
        "      as_supervised=True,\n",
        "  )\n",
        "\n",
        "  return [split.shuffle(1024) for split in splits]"
      ],
      "metadata": {
        "id": "fS9bAFV1eMyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = load_mnist()"
      ],
      "metadata": {
        "id": "ikmM78v0kaAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 - Resize and Normalize Data for Row Wise Experiments"
      ],
      "metadata": {
        "id": "W_7sseQeTU0N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The train and test data is now stored as `tf.data.Dataset` objects. Let's define a model to augment data by random image transformations.\n",
        "\n",
        "**TASK**: Complete the function `build_augmentation_model`. The augmentation model shall\n",
        "- randomly translate (shift) an image by up to 20% in either direction along the height and width,\n",
        "- randomly rotate an image by up to 36° in either direction.\n",
        "\n",
        "Points **outside the boundaries** of the input shall be **filled with `constant` values**. If you are unsure which layer to use, check the TF documentation on [augmentation layers](https://www.tensorflow.org/guide/keras/preprocessing_layers#image_preprocessing)."
      ],
      "metadata": {
        "id": "O1-swajHkL29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: build_augmentation_model (3 points)\n",
        "\n",
        "from tensorflow.keras import layers, Sequential\n",
        "\n",
        "def build_augmentation_model():\n",
        "\n",
        "  ### START YOUR CODE HERE ###\n",
        "  augmentation_model = \n",
        "  ### END YOUR CODE HERE ###\n",
        "\n",
        "  return augmentation_model"
      ],
      "metadata": {
        "id": "HuLW1oWjhlkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augmentation_model = build_augmentation_model()\n",
        "\n",
        "utils.plot_augmentation_images(train_data, augmentation_model)"
      ],
      "metadata": {
        "id": "AGcYkanRjL8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check the shape of one sample:"
      ],
      "metadata": {
        "id": "40LteskTqCtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Sample shape: {train_data.element_spec[0].shape}')"
      ],
      "metadata": {
        "id": "263Sqf0fkwyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see a shape of `(28, 28, 1)`, i.e., a grayscale image with 28 by 28 pixels. In order to save computing time, you should **rescale the images to 14 by 14 pixels**. In addition, you need to **cast the pixel values to `float32` type and normalize their values to the interval `[0., 1.]`**.\n",
        "\n",
        "To apply transformations to each element of a `Dataset` object, you can simply wrap the transformations in a function, e.g., `map_func`, and use the `map` method:\n",
        "\n",
        "```python\n",
        "def map_func(image, label) # the dataset is supervised, hence each sample has input `image` and output `label`\n",
        "  ... any operation ...\n",
        "  return image, label\n",
        "\n",
        "dataset = dataset.map(map_func)\n",
        "```\n",
        "\n",
        "Note that you can stack calls to `map`:\n",
        "\n",
        "```python\n",
        "def map_func(image, label) \n",
        "  ... any operation ...\n",
        "  return image, label\n",
        "\n",
        "def another_map_func(image, label) \n",
        "  ... any other operation ...\n",
        "  return image, label\n",
        "\n",
        "dataset = dataset.map(map_func).map(another_map_func)\n",
        "```\n",
        "\n",
        "**TASKS**: \n",
        "- Create a `resize_img` function to resize each image to 14 $\\times$ 14 pixels. You may want to use the [`tf.image.resize`](https://www.tensorflow.org/api_docs/python/tf/image/resize) method for resizing. \n",
        "\n",
        "- Also create a `normalize_img` function to cast the pixel values to `tf.float32` type and normalize the values to the range `[0., 1.]`. Check out the [`tf.cast`](https://www.tensorflow.org/api_docs/python/tf/cast) method. \n",
        "\n",
        "- `map` your functions to the train and test data in the `load_row_class_data` function."
      ],
      "metadata": {
        "id": "VQHmh89Omaj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: resize_img (1 point)\n",
        "# GRADED FUNCTION: normalize_img (1 point)\n",
        "# GRADED FUNCTION: load_row_class_data (2 points)\n",
        "\n",
        "BATCH_SIZE = 512\n",
        "\n",
        "### START YOUR CODE HERE ### ( ≈ 6 LOC)\n",
        "\n",
        "### END YOUR CODE HERE ###\n",
        "\n",
        "\n",
        "def load_row_class_data(augment=True):\n",
        "  \n",
        "  # load the mnist dataset\n",
        "  train_data, test_data = load_mnist()\n",
        "  test_data = test_data.take(1000) # test only on 1k samples\n",
        "\n",
        "  if augment:\n",
        "    train_data = train_data.map(lambda x, y: (augmentation_model(x), y))\n",
        "  \n",
        "  ### START YOUR CODE HERE ### ( ≈4 LOC)\n",
        "  # resize and normalize the train images\n",
        "\n",
        "  # resize and normalize the test images\n",
        "\n",
        "  ### END YOUR CODE HERE ###\n",
        "  \n",
        "  # create batches\n",
        "  train_data = train_data.cache().batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  test_data = test_data.cache().batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "  return train_data, test_data"
      ],
      "metadata": {
        "id": "FZdGn09_vUvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now load the processed dataset and take a look on a sample. The shape of each batch of this dataset should be `(None, 14, 14, 1)`. The plotted image should display a digit.\n",
        "\n",
        "**NOTE**: If your augmentation model does not work, set the `augment` argument to `False`."
      ],
      "metadata": {
        "id": "y9lDcUxj3rUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row_train_data, row_test_data = load_row_class_data(augment=True)\n",
        "\n",
        "print(f'Batch shape (row wise data): {row_train_data.element_spec[0].shape}')\n",
        "\n",
        "utils.show_dataset_sample(row_train_data)"
      ],
      "metadata": {
        "id": "4D4XAifz3lMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 - Reshape Data for Pixel by Pixel Experiments\n",
        "\n",
        "In order to forward the image through the RNN as a sequence of pixels, you need to reshape each image to the shape `( width * height, 1)`.\n",
        "\n",
        "**TASKS**: \n",
        "- Create a `reshape_img` function to reshape each image to `( width * height, 1)`. You may want to use the [`tf.reshape`](https://www.tensorflow.org/api_docs/python/tf/reshape) method.\n",
        "\n",
        "- Map your functions to the train and test data in the `load_pixel_class_data` function.\n",
        "\n"
      ],
      "metadata": {
        "id": "OqF3OKEWTqSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: reshape_img (1 point)\n",
        "# GRADED FUNCTION: load_px_class_data (1 point)\n",
        "\n",
        "### START YOUR CODE HERE ### ( ≈ 3 LOC)\n",
        "\n",
        "### END YOUR CODE HERE ###\n",
        "\n",
        "def load_px_class_data(augment=True):\n",
        "\n",
        "  # load the mnist dataset\n",
        "  train_data, test_data = load_mnist()\n",
        "  test_data = test_data.take(1000) # test only on 1k samples\n",
        "\n",
        "  if augment:\n",
        "    train_data = train_data.map(lambda x, y: (augmentation_model(x), y))\n",
        "\n",
        "  ### START YOUR CODE HERE ### ( ≈ 6 LOC)\n",
        "  # resize, normalize, and reshape the train images\n",
        "  \n",
        "  # resize, normalize, and reshape the test images\n",
        "\n",
        "  ### END YOUR CODE HERE ###\n",
        "  \n",
        "  # create batches\n",
        "  train_data = train_data.cache().batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  test_data = test_data.cache().batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "  return train_data, test_data"
      ],
      "metadata": {
        "id": "qMqGQF-tYd0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now load the processed pixel wise dataset and take a look on a sample. The shape of a batch of reshaped samples should be `(None, 196, 1)`. The plotted image should still display a digit because the data is reshaped back in the plotting function."
      ],
      "metadata": {
        "id": "BoKPaO9xUARW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "px_train_data, px_test_data = load_px_class_data(augment=True)\n",
        "\n",
        "print(f'Batch shape (pixel wise data): {px_train_data.element_spec[0].shape}')\n",
        "\n",
        "utils.show_dataset_sample(px_train_data)"
      ],
      "metadata": {
        "id": "2PabmlT_UARX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you did everything right, you should now have **two variants of the MNIST dataset**:\n",
        "1. `(px_train_data, px_test_data)` for forwarding the image **pixel by pixel** through the RNN. Therefore, a batch must have the shape `(None, 196, 1)`. You will use this dataset as a sequence with **196 timesteps** and **1 feature per timestep**.\n",
        "2. `(row_train_data, row_test_data)` for forwarding the image **row by row** through the RNN. Therefore, a batch must have the shape `(None, 14, 14, 1)`. You will use this dataset as a sequence with **14 timesteps** and **14 features per timestep**."
      ],
      "metadata": {
        "id": "lQwwBTKXUwMu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 - Build Model"
      ],
      "metadata": {
        "id": "28wiwjaPda1T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it's time to build your model. We suggest you to build a stacked RNN with three [**LSTM** layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM) and one **Dense** layer for classification. All LSTM layers shall have the same `hidden_size`, i.e., number of units. \n",
        "Note that the second and third LSTM layers in the stack expect an input for each timestep. Hence, the first and second LSTM layers must return an output at each timestep.\n",
        "\n",
        "**TASK**: Complete the function `build_classification_model` by designing a classification RNN with three LSTM layers and one Dense layer. The model compilation will be done later."
      ],
      "metadata": {
        "id": "e2-xXqx1ed3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: build_classification_model (2 points)\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def build_classification_model(\n",
        "    input_timesteps: int, \n",
        "    input_features_size: int, \n",
        "    hidden_size: int, \n",
        "    num_classes: int,\n",
        "    summary: bool=True,\n",
        "    ):\n",
        "  \"\"\"Build LSTM-based model for sequence classification.\n",
        "\n",
        "  Args:\n",
        "    input_timesteps (int): number of input timesteps \n",
        "    input_features_size (int): number of input features per timestep\n",
        "    hidden_size (int): hidden size of the LSTM (=number of units), \n",
        "    num_classes (int): number of classes,\n",
        "    augmentation_model (Model): keras model for augmentation (default: None),\n",
        "    summary (bool=True): print model summary\n",
        "\n",
        "  Returns:\n",
        "      model (tf.keras.Model): built TensorFlow Keras Model\n",
        "  \"\"\"\n",
        "  \n",
        "  ### START YOUR CODE HERE ### (≈ 6 LOC)\n",
        "  \n",
        "  ### END YOUR CODE HERE ###\n",
        "  \n",
        "  if summary:\n",
        "    print(your_model.summary())\n",
        "\n",
        "  return your_model"
      ],
      "metadata": {
        "id": "5HVPvOvkMJBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test your function by creating an instance of your model. Think about the correct values for the `input_timesteps`, `input_features_size` for the pixel by pixel data."
      ],
      "metadata": {
        "id": "VftuRLoffGgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### START YOUR CODE HERE ### (1 LOC)\n",
        "\n",
        "### END YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "bzglGx0ldG9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 - Train and Evaluate Pixel by Pixel Model"
      ],
      "metadata": {
        "id": "deM7jNbvNGKR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 - Callbacks\n",
        "\n",
        "The next cells sets up callbacks to reduce the learning rate and to stop the training if the model does not improve anymore:"
      ],
      "metadata": {
        "id": "ZPhAtCu0WHma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "def set_up_callbacks():\n",
        "\n",
        "  reduce_lr_on_plateau_callback = ReduceLROnPlateau(\n",
        "      monitor='val_loss',\n",
        "      factor=0.2,\n",
        "      patience=10,\n",
        "      min_lr=0.0001,\n",
        "      verbose=1\n",
        "  )\n",
        "\n",
        "  early_stopping_callback = EarlyStopping(\n",
        "      monitor='val_loss',\n",
        "      patience=20,\n",
        "      restore_best_weights=True,\n",
        "      verbose=1\n",
        "  )\n",
        "\n",
        "  return reduce_lr_on_plateau_callback, early_stopping_callback"
      ],
      "metadata": {
        "id": "WkvDOk7qebn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 - Model Training\n",
        "\n",
        "Now it's time to create an instance of your model and start the training. Start with the pixel by pixel model and train the model on the `px_train_data`. We suggest to set `hidden_size = 8`, but you can choose other values if you like.\n",
        "Think about the correct values for `input_timesteps` (= number of timesteps) and `feature_size` (= number of features per timestep).\n",
        "\n",
        "**TASK**: Complete the function `run_px_classification_experiment` to perform the pixel by pixel classification experiments.\n",
        "- Load the pixel by pixel dataset.\n",
        "- Create an instance of your model from Section 1.2.\n",
        "- Compile the model using `SparseCategoricalCrossentropy` loss, `RMSprop` optimizer with a learning rate of $10^{-2}$, `accuracy` as metric.\n",
        "- Train the model for `num_epochs` epochs.\n",
        "- Don't forget the callbacks.\n",
        "- Return the training history and trained model."
      ],
      "metadata": {
        "id": "-vUrXzM8QfXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: run_px_classification_experiment (5 points)\n",
        "\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "\n",
        "def run_px_classification_experiment(num_epochs :int=100):\n",
        "\n",
        "  ### START YOUR CODE HERE ### (≈ 5 LOC)\n",
        "\n",
        "  # load data\n",
        "  \n",
        "  # build model\n",
        "\n",
        "  # compile model\n",
        "\n",
        "  # train model\n",
        "\n",
        "  # return model and history\n",
        "  \n",
        "  ### END YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "I0XfPehbxfoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now run the experiment. Your model should reach a validation accuracy > 50% within the first ten epochs. After 50 epochs the validation accuracy should be >80%:"
      ],
      "metadata": {
        "id": "pHbbEx30kOit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "px_class_model, px_class_history = run_px_classification_experiment(num_epochs=100)\n",
        "utils.plot_history(px_class_history)"
      ],
      "metadata": {
        "id": "U1XtPmgHguJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 - Investigate Continuous Model Predictions\n",
        "\n",
        "Instead of performing the classification based on the LSTM cell's output at the end of the sequence, you can also analyze your models' predictions at each timestep. \n",
        "\n",
        "To do so, you only need to build a model with essentially the same architecture and ensure that each LSTM layer returns an output at each timestep. Then the classifier receives an input at each timestep and returns a prediction accordingly.\n",
        "\n",
        "**TASK**: Create a model with same architecture as in Section 1.2 but returning a prediction at each timestep.\n"
      ],
      "metadata": {
        "id": "1AkvG440s-sL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: build_continuous_classification_model (1 point)\n",
        "\n",
        "def build_continuous_classification_model(\n",
        "    input_timesteps: int, \n",
        "    input_features_size: int, \n",
        "    hidden_size: int, \n",
        "    num_classes: int,\n",
        "    summary: bool=True,\n",
        "    ):\n",
        "  \"\"\"Build LSTM-based model for sequence classification.\n",
        "\n",
        "  Args:\n",
        "    input_timesteps (int): number of input timesteps \n",
        "    input_features_size (int): number of input features per timestep\n",
        "    hidden_size (int): hidden size of the LSTM (=number of units), \n",
        "    num_classes (int): number of classes,\n",
        "    summary (bool=True): print model summary\n",
        "\n",
        "  Returns:\n",
        "      model (tf.keras.Model): built TensorFlow Keras Model\n",
        "  \"\"\"\n",
        "  \n",
        "  ### START YOUR CODE HERE ### (≈ 6 LOC)\n",
        "  \n",
        "  ### END YOUR CODE HERE ###\n",
        "  \n",
        "  if summary:\n",
        "    print(your_model.summary())\n",
        "\n",
        "  return your_model"
      ],
      "metadata": {
        "id": "cRQdClSVouEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, create an instance of the continuous pixel by pixel model `px_class_model_continuous` and transfer the weights from the trained `px_class_model`:"
      ],
      "metadata": {
        "id": "1XPDrFa3o8AR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### START YOUR CODE HERE ### (1 LOC)\n",
        "px_class_model_continuous = build_continuous_classification_model(\n",
        "    \n",
        ")\n",
        "### END YOUR CODE HERE ###\n",
        "\n",
        "# transfer weights from trained model\n",
        "px_class_model_continuous.set_weights(px_class_model.get_weights())"
      ],
      "metadata": {
        "id": "6N4e70SWpg99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute the cell below to take one sample from the test dataset and forward propagate it through the network. If everything is correct, the number of predictions is equal to the number of timesteps:"
      ],
      "metadata": {
        "id": "mL2ICp-VOpwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_idx = 42\n",
        "\n",
        "px_sample = utils.get_dataset_sample(px_test_data, sample_idx)\n",
        "continuous_predictions = np.array(px_class_model_continuous.predict( np.expand_dims(px_sample[0], 0)))[0,:]\n",
        "\n",
        "print(f'Number of predictions: {len(continuous_predictions)}')"
      ],
      "metadata": {
        "id": "aSmDO4k6psSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the `plot_continuous_classification` function defined above to create an interactive plot of the probability distribution and processed image. Use the slider to navigate through the sequence. Can you predict the correct digit before your RNN?"
      ],
      "metadata": {
        "id": "Yc1hdC8WPF6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "utils.plot_continuous_classification(px_sample[0], continuous_predictions)"
      ],
      "metadata": {
        "id": "_VXQonP2Mxf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 - Train and Evaluate Row by Row Model"
      ],
      "metadata": {
        "id": "S9x_zSQPdXuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 - Model Training\n",
        "\n",
        "You should have observed that the pixel by pixel model can make some erratic predictions with progressing timesteps. Of course, memorizing a long series of mostly zero pixel values can be a complicated task.\n",
        "\n",
        "If you process the image row by row, more context is added for each timestep and we can expect better model performance. Next, you shall train a row-wise model to evaluate this hypothesis.\n",
        "\n",
        "**TASK**: Complete the function `run_row_classification_experiment` to perform the row by row classification experiments.\n",
        "- Load the row by row dataset.\n",
        "- Create an instance of your model from Section 1.2. Think about the correct number of time steps and features per time step.\n",
        "- Compile the model using `SparseCategoricalCrossentropy` loss, `RMSprop` optimizer with a learning rate of $10^{-2}$, `accuracy` as metric.\n",
        "- Train the model for `num_epochs` epochs. Don't forget your callbacks!\n",
        "- Return the training history and trained model."
      ],
      "metadata": {
        "id": "vgMW-iT0QHSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: run_row_classification_experiment (1 point)\n",
        "\n",
        "def run_row_classification_experiment(num_epochs :int=100):\n",
        "\n",
        "  ### START YOUR CODE HERE ### (≈ 5 LOC)\n",
        "\n",
        "  # load data\n",
        "\n",
        "  # build model\n",
        "  \n",
        "  # compile model\n",
        "\n",
        "  # train model\n",
        "\n",
        "  # return model and history\n",
        "  \n",
        "  ### END YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "e9x3J99DYDxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now run the experiment. Your model should reach a validation accuracy > 90% within the first ten epochs."
      ],
      "metadata": {
        "id": "7C2hLV5oY8eI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row_class_model, row_class_history = run_row_classification_experiment()\n",
        "utils.plot_history(row_class_history)"
      ],
      "metadata": {
        "id": "e6X_MDTiY8eQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 - Investigate Continuous Model Predictions\n",
        "\n",
        "Same as for the pixel wise model, let's analyse the prediction at each time step.\n",
        "\n",
        "**TASK**: Create an instance of the continuous row by row model `row_class_model_continuous`."
      ],
      "metadata": {
        "id": "iYsMsDj1jWc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### START YOUR CODE HERE ### (1 LOC)\n",
        "row_class_model_continuous = build_continuous_classification_model(\n",
        "    \n",
        ")\n",
        "### END YOUR CODE HERE ###\n",
        "\n",
        "# transfer weights from trained model\n",
        "row_class_model_continuous.set_weights(row_class_model.get_weights())"
      ],
      "metadata": {
        "id": "gKNePTrCZfW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute the cell below to take one sample from the test dataset and forward propagate it through the network. If everything is correct, the number of predictions is equal to the number of timesteps:"
      ],
      "metadata": {
        "id": "02eLK-wLj9F0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_idx = 42\n",
        "\n",
        "row_sample = utils.get_dataset_sample(row_test_data, sample_idx)\n",
        "continuous_predictions = np.array(row_class_model_continuous.predict( np.expand_dims(row_sample[0], 0)))[0,:]\n",
        "\n",
        "print(f'Number of predictions: {len(continuous_predictions)}')"
      ],
      "metadata": {
        "id": "OQBe2sTFj9F0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And plot the result:"
      ],
      "metadata": {
        "id": "gFCh2oIXj9F0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "utils.plot_continuous_classification(row_sample[0], continuous_predictions)"
      ],
      "metadata": {
        "id": "NgbDQeMUZ0HL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "\n",
        "**TASK**: Compare the number of parameters of the first LSTM layer in the pixel by pixel model to the number of parameters of the first LSTM layer in the row by row model. Explain how the number of parameters is calculated and why they are different in the two models.\n",
        "\n",
        "# **GRADED RESPONSE**: (2 points)\n",
        "*ADD YOUR ANSWER HERE*"
      ],
      "metadata": {
        "id": "N5cT72s7ds8O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Congratulations on completing Assignment 3!\n",
        "\n",
        "Complete the steps below for submission.\n",
        "\n",
        "# Submission Instructions\n",
        "\n",
        "You may now submit your notebook to moodle:\n",
        "- Save the notebook (`CTRL`+ `s` or '*File*' -> '*Save*')\n",
        "- Click on '*File*' -> '*Download .ipynb*' for downloading the notebook as IPython Notebook file.\n",
        "- Upload the downloaded IPython Notebook file to **Moodle**."
      ],
      "metadata": {
        "id": "4Y2mBy5Mdpia"
      }
    }
  ]
}